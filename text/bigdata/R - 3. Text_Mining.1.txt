
1. 의미: 문자 데이터에서 -> 가치있는 정보를 추출해내는 분석기법

2. 분석방법: 핵심은 '형태소 분석'(Morphology Analysis)
   - 텍스트에서 -> 어절들의 품사를 파악
   - 명사, 동사, 형용사 등 의미를 지닌 품사별로 단어들 추출
   - 각 단어의 출현빈도 확인

3. 분석목적
   - 소셜네트워크(SNS) / 웹사이트의 글을 분석
   - 주어진 텍스트의 내용(=사람들이 무슨 이야기를 나누고 있는지) 파악

4. 단계별 수행방법
   (1) 관련 패키지 준비 (Pacakge의 대소문자구분 주의)
   	   - KoNLP : 한글 자연어 분석 R 패키지
   	   - Java JDK(or JRE) 설치
   	   - KoNLP Dependency Package: rJava, memoise

   (2) 관련 패키지 설치
   	   # Sys.getenv() / Sys.setenv() 함수: 'base' environment에 존재
   	   > Sys.getenv("JAVA_HOME")		# Check Java JDK(or JRE) install folder
   	   > Sys.setenv(JAVA_HOME="C:/Java/jdk1.8.0_152/")	# if 'JAVA_HOME' env NOT set, do it!!

   	   > install.packages('rJava')			# KoNLP 패키지의 의존성 패키지
   	   > install.packages('memoise')		# KoNLP 패키지의 의존성 패키지
   	   > install.packages('KoNLP')			# 한글 형태소 분석
   	   > install.packages('stringr')		# 문자열 조작
   	   > 

   (3) 관련 패키지 로딩
   	   > library(KoNLP)
   	   > library(dplyr)
   	   > library(stringr)
   	   >

   (4) 형태소 분석용 사전 준비(설정)
       - '한국정보화진흥원(NIA)'이 공개한, 한글 형태소 사전(약 98만개 단어로 구성) 사용
       - 형태소 분석시 사용
       	> useNIADic()
		Backup was just finished!
		Downloading package from url: https://github.com/haven-jeon/NIADic/releases/download/0.0.1/NIADic_0.0.1.tar.gz
		Installing NIADic
		"C:/R/R-34~1.2/bin/x64/R" --no-site-file --no-environ --no-save --no-restore --quiet  \
		  CMD build  \
		  "C:\Users\trinity\AppData\Local\Temp\RtmpsJFM5Z\devtools3348e59120b\NIADic"  \
		  --no-resave-data --no-manual 

		* checking for file 'C:\Users\trinity\AppData\Local\Temp\RtmpsJFM5Z\devtools3348e59120b\NIADic/DESCRIPTION' ... OK
		* preparing 'NIADic':
		* checking DESCRIPTION meta-information ... OK
		* installing the package to build vignettes
		* creating vignettes ... OK
		경고: 'inst/doc' files
		    'insighter-dic.Rmd', 'woorimalsam-dic.Rmd', 'insighter-dic.html',  'woorimalsam-dic.html', 'insighter-dic.R', 'woorimalsam-dic.R'
		  ignored as vignettes have been rebuilt.
		  Run R CMD build with --no-build-vignettes to prevent rebuilding.
		* checking for LF line-endings in source and make files and shell scripts
		* checking for empty or unneeded directories
		* building 'NIADic_0.0.1.tar.gz'

		"C:/R/R-34~1.2/bin/x64/R" --no-site-file --no-environ --no-save --no-restore --quiet  \
		  CMD INSTALL "C:/Users/trinity/AppData/Local/Temp/RtmpsJFM5Z/NIADic_0.0.1.tar.gz"  \
		  --library="C:/Users/trinity/Documents/R/win-library/3.4" --install-tests 

		* installing *source* package 'NIADic' ...
		** R
		** inst
		** preparing package for lazy loading
		** help
		*** installing help indices
		** building package indices
		** installing vignettes
		** testing if installed package can be loaded
		* DONE (NIADic)
		983012 words dictionary was built.
		>

   (5) 분석 데이터 준비
   		# News.txt 파일의 위치(경로) 확인할 것
   		# readLines() function: 'base' environment에 존재
   		> text <- readLines('wordcloud/News.txt')
		Warning message:	# 아래 경고 발생시, 읽어드릴 파일의 맨 마지막 라인 끝에서 엔터 키 한번 입력해줄 것
		In readLines("wordcloud/News.txt") :
		  incomplete final line found on 'wordcloud/News.txt'
		> text <- readLines('wordcloud/News.txt')
		> print(text)
		> 

/*
   (6) 특수문자 제거 등 데이터 정제
   		#> install.packages('stringr')
   		#> library(stringr)

   		# '\\W': 정규표현식 - 모든 특수문자 지칭
		> news <- str_replace_all(news, '\\W', '')	# 텍스트에서 모든 특수문자 제거 											
		> View(news)
		> 
*/

   (7) 분석 수행 - 출현빈도가 가장 높은 단어 추출

       1) 명사 추출 -> 주어진 데이터(텍스트)의 내용 파악
		# 설치된 KoNLP 패키지가 제대로 되어있는지 간단히 확인
		> extractNoun('특수문자 제거 등 데이터 정제')
		[1] "특수"   "문자"   "제거"   "등"     "데이터" "제" 

       2) 단어 출현 빈도표(R data frame 유형으로) 작성
		>    
		> nouns.list <- extractNoun(text)
		# 아래 경고 메시지 발생시, extractNoun() 함수 재수행
		...
		경고 메시지: 
		1: In readLines("wordcloud/news.txt") : line 1 appears to contain an embedded nul
		2: In readLines("wordcloud/news.txt") : line 2 appears to contain an embedded nul
		3: In readLines("wordcloud/news.txt") : line 3 appears to contain an embedded nul
		...
		> print(nouns.list)
		> str(nouns.list)      		# List Data Structure
		> new.nouns.list = list()
		> str(new.nouns.list)
		> 
		> idx = 0
		> for( nouns in nouns.list ) {
    		+	if( length(nouns) > 1 ) {       # remove "" (empty string)
        	+		idx = idx + 1
        	+		nouns <- str_replace_all(nouns, "\\W", "")
        	+		nouns <- str_trim(nouns)
        	+		# print(nouns)              # nouns: character type vector
        	+		new.nouns.list[[idx]] = nouns
    		+	}
		+ }
		>
		> print(new.nouns.list)
		> 
		> mode( unlist( new.nouns.list ) )	# unlist() function: 'list' -> 'vector' 로 변환
		[1] "character"
		> str( unlist( new.nouns.list ) )
 		chr [1:389] "다주택자" "선택지" "여력" "청년층" "주거안정·내집마련" "여건" "확대" "" ...
		>
		> wordcount <- table( unlist( new.nouns.list ) )
		> print( wordcount )
		> df_wordcount <- as.data.frame( wordcount, stringsAsFactors=F )
		> print( df_wordcount )
		> str( df_wordcount )
		'data.frame':	229 obs. of  2 variables:
		 $ Var1: chr  "" "'10·24" "'평창2018'"  ...
		 $ Freq: int  19 1 1 1 1 3 2 1 1 2 ...
		> df_wordcount <- rename( df_wordcount, word=Var1, cnt=Freq )
		> print(df_wordcount)
		> 

       3) 이중 가장많이 출현한 단어의 빈도표 작성
		>
		> class( df_wordcount )
		[1] "data.frame"
		> str( df_wordcount )
		'data.frame':	229 obs. of  2 variables:
		 $ word: chr  "" "'10·24" "'평창2018'" "(세종=뉴스1)" ...
		 $ cnt : int  19 1 1 1 1 3 2 1 1 2 ...
		> 
		> nchar( df_wordcount )
		word  cnt 
		1524  691 
		> str( nchar( df_wordcount ) )
		 Named int [1:2] 1524 691
		 - attr(*, "names")= chr [1:2] "word" "cnt"
		> nchar(df_wordcount$word )				# base::nchar()
		  [1]  0  6  8  8  7  1  1  3  2  1  4  2  5  2  2  3  1  3 20  5  2  2  4  3  3  3  2  2  2  1  2  1
		 [33]  2  3  2  2  2  3  2  2  2  2  2  2  4  3  2  4  4  2  1  2  2  2  3  2  2  3  1  5 13  2  4  1
		 [65]  1  2  2  2  2  2  3  2  2  2  1  2  1  1  3  1  5  2  2  2  1  1  2  2  3  2  2  2  1  3  1  2
		 [97]  2  2  2  1  2  3  2  3  1  2  3  2  2  2  2  1  3  2  2  7  3  3  2  2  1  2  2  1  2  3  6  2
		[129]  2  4  2  2  1  5  2  7  3  4  2  2  1  3  2  2  2  4  2  2  2  2  4  1  3  1  2  3  2  2  1  2
		[161]  2  2  2  2  4  4  2  3  4  2  2  3  2  2  2  2  2  2  2  2  2 13  2  4  2  4  9  3  2  2  2  4
		[193]  1  2  2  2  3  2  4  2  2  1  1  2  2  3  9  2  2  1  2 11  4  4  2  2  1  2  2  1  2  2  1  2
		[225]  2  2  2  2  2
		> df_keywords <- filter(df_wordcount, nchar(word) > 2)	# base::filter()
		# 아래 경고 메시지 발생시, filter() 함수 재수행
		Warning message:	
		In get(results[[i]], pos = which(search() == packages[[i]])) :
		  restarting interrupted promise evaluation
		> print( df_keywords )
		> str( df_keywords )
		'data.frame':	68 obs. of  2 variables:
		 $ word: chr  "'10·24" "'평창2018'" ...
		 $ cnt : int  1 1 1 1 1 1 1 1 4 1 ...
		> 

       4) 빈도표 확인
		  - 출현빈도가 높은 단어로 정렬후, 30개 단어 저장
		  >
		  > top30 <- df_keywords %>% arrange(desc(cnt)) %>% head(30)
		  > class( top30 )
		  [1] "data.frame"
		  > str( top30 )
		  'data.frame':	30 obs. of  2 variables:
		   $ word: chr  "다주택자" "DTI" "무주택" "부동산" ...
		   $ cnt : int  9 4 4 4 4 3 3 2 2 2 ...
		  > print(top30)
		  > 


   (8) 워드 클라우드(Word Cloud)
   	   - 추출한 단어의 출현빈도 -> 구름모양으로 표현한 그래프
   	   - 출현 단어의 빈도에 따라 -> 글자 크기와 색깔이 다르게 표현
   	   - 무슨 단어가 얼마나 많이 사용되었는지를 파악 수월
   	   - 빈도가 높은 단어는, plot에 크고 중앙에 배치
   	   - 빈도가 낮은 단어는, plot에서 클라우드의 바깥쪽에 배치
   	   - 수행방법:

   	   	1) 관련 패키지 준비 	- 'wordcloud' package
		> 
		> install.packages('wordcloud')		# word cloud 생성 패키지
		> library(wordcloud)
		> library(RColorBrewer)			# R built-in package
							# word cloud에 사용될 글자 색상 표현에 사용
		> 

   	   	2) 단어 색깔 지정 - 색상코드목록(Hex Color Code) 생성
		> color.pal <- brewer.pal(7, 'Dark2')	# RColorBrewer package
		> class( color.pal )
		[1] "character"
		> str( color.pal )
		 chr [1:7] "#1B9E77" "#D95F02" "#7570B3" "#E7298A" "#66A61E" "#E6AB02" "#A6761D"
		> print( color.pal )
		> 

   	   	3) 무작위 수(= 난수, Random Number)를 이용 - 수행시마다, 다른 모양의 워드 클라우드 생성
		> 
		> set.seed( sample(1:30, 1) )		# in 'base' environment
		> 

   	   	4) 워드 클라우드 생성
		>
		> str( df_keywords )
		'data.frame':	68 obs. of  2 variables:
		 $ word: chr  "'10·24" "'평창2018'"  ...
		 $ cnt : int  1 1 1 1 1 1 1 1 4 1 ...
		> wordcloud(
		    words=df_keywords$word,			# 표현할 단어들
		    freq=df_keywords$cnt,			# 표현할 단어들의 빈도
		    random.order = F,				# 높은 빈도를 plot 중앙에 배치
		    rot.per = .1,				# 회전표현할 단어의 비율
		    min.freq = 1,				# 최소 단어 빈도
		    max.words = 300,				# 최대 표현할 단어의 개수
		    scale = c(6, .2),				# 표현 단어의 크기 조정
		    colors = color.pal				# RColorRGB 패키지로 생성한, 색상팔레트
		)
		> 

   	   	5) 단어 색상 설정 - 빈도가 높은 단어의 색상을 지정
		> 
		> set.seed( sample(1:30, 1) )				# in 'base' environment
		> color.pal <- brewer.pal(7, 'Reds')[4:7]	# RColorBrewer package
		> str(color.pal)
 		chr [1:4] "#FB6A4A" "#EF3B2C" "#CB181D" "#99000D"
		> 
		> wordcloud(
		    words=df_keywords$word,
		    freq=df_keywords$cnt,
		    random.order = F,
		    rot.per = .1,
		    min.freq = 1,
		    max.words = 200,
		    scale = c(6, .2),
		    colors = color.pal
		  )
		>

	(9) 수행 함수호출 요약표
		> library(KoNLP)
		> library(dplyr)
		> library(stringr)
		> 
		> useNIADic()
		> 
		> text <- readLines('wordcloud/News.txt')	# News.txt 파일은 'ANSI' 인코딩하여 저장
		> print( text )
		> 
		> nouns.list <- extractNoun(text)
		> print(nouns.list)
		> str(nouns.list)      # List Data Structure
		> 
		> new.nouns.list = list()
		> str(new.nouns.list)
		> 
		> idx = 0
		> for( nouns in nouns.list ) {
    		+	if( length(nouns) > 1 ) {	# remove "" (empty string)
        	+		idx = idx + 1
        	+		nouns <- str_replace_all(nouns, "\\W", "")
        	+		nouns <- str_trim(nouns)
        	+		# print(nouns)      	# nouns: character type vector
        	+		new.nouns.list[[idx]] = nouns
    		+	}
		+ }
		>
		> print(new.nouns.list)
		> 
		> wordcount <- table( unlist(new.nouns.list) )
		> print(wordcount)
		> 
		> df_wordcount <- as.data.frame( wordcount, stringsAsFactors=F )
		> print(df_wordcount)
		> 
		> df_wordcount <- rename(df_wordcount, word=Var1, cnt=Freq)
		> print(df_wordcount)
		> 
		> df_keywords <- filter(df_wordcount, nchar(word) > 2)
		> print(df_keywords)
		> 
		> top30 <- df_keywords %>% arrange(desc(cnt)) %>% head(30)
		> print(top30)
		> 
		> color.pal <- brewer.pal(8, 'Dark2')[5:8]
		> print(color.pal)
		> 
		> set.seed( sample(1:30, 1) )
		> 
		> wordcloud(
    			words=df_keywords$word,
    			freq=df_keywords$cnt,
    			random.order = F,
    			rot.per = .1,
    			min.freq = 1,
    			max.words = 200,
    			scale = c(6, .2),
    			colors = color.pal
		  )
		>


5. 에러대응
   (1) 'rJava' 패키지 로드시, 아래 에러 발생하면 다음과 같이 대응함
       - 에러: Error: .onLoad failed in loadNamespace() for rJava
       - 대응: Java JDK 설치폴더 설정(또는 확인)후, 재 로딩 수행
               만일 JDK Home이, "C:/Program Files/Java/jre1.8.0_111/" 이라면
               > Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_111/")

   (2) 준비한 분석대상 텍스트 파일 읽기 수행시, 아래의 경고 메시지 발생하면,
   		# News.txt 파일의 위치(경로) 확인할 것
   		# readLines() function: 'base' environment에 존재
   		> news <- readLines('wordcloud/News.txt')
		Warning message:	
		In readLines("wordcloud/News.txt") :
		  incomplete final line found on 'wordcloud/News.txt'
		> news <- readLines('wordcloud/News.txt')
		> View(news)
		> 

		"위 경고 발생시, 읽어드릴 파일의 맨 마지막 라인 끝에서 엔터 키 한번 입력해줄 것"

	(3) 에러 메시지가 '한글'로 출력될 경우 -> '영문'으로 출력되도록 아래 함수 호출 후, 재 수행
		>
		> Sys.setenv('LANGUAGE'='EN')	# 'base' environment에 존재하는 함수
		> 

	(4) 읽어온 텍스트 데이터 파일의 내용을 View() 함수로 확인시, 문자가 깨지는 경우
	    - 대상 텍스트 파일을 문서 편집기(예: Notepad 등)에서 파일 open
	    - Encoding 방식 -> 'ANSI'로 변경 (이전의 인코딩 방식은, 'utf8' or 'Unicode' 일 것임)
	    - 다시 readLines(), View() 함수 호출 수행

	(5) > konlp_nouns <- extractNoun(news) 함수 호출시, 아래 경고 제거 방법
	    - 읽어드린 텍스트 데이터 파일을, 편집기(예: 윈도우 notepad)에서 인코딩 방식을
	    - 기존 'ANSI' -> 'utf8 or unicode' 로 저장 -> 다시, 'ANSI'로 저장후 호출함수 재수행
		...
		경고 메시지: 
		1: In readLines("wordcloud/news.txt") : line 1 appears to contain an embedded nul
		2: In readLines("wordcloud/news.txt") : line 2 appears to contain an embedded nul
		3: In readLines("wordcloud/news.txt") : line 3 appears to contain an embedded nul
		...


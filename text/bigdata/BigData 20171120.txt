빅데이터

-----------------------------------------------------------------
$ scrapy genspider --force --template=basic spidername milemoa.com		# Edit spider after creating it
$ scrapy genspider --force -t basic spidername milemoa.com

$ cd /home/hadoop/scrapy/	(NOK)
$ cd /home/hadoop/scrapy/ProjectName/	(ok)
$ cd /home/hadoop/scrapy/ProjectName/ProjectName/	(ok)
$ cd /home/hadoop/scrapy/ProjectName/ProjectName/spiders/	(ok)
------------------------------------------------------------------
Created spider 'spidername' using template 'basic' in module:
  ProjectName.spiders.spidername
  
** Created py : /home/hadoop/scrapy/ProjectName/ProjectName/spiders/spidername.py **

1) Required Components
   - name : Spider's Name (must be Unique in the project )
   - start_urls : Starting URLs for Crawling
   - parse() : return the scraped data of response for each URLs


2) Programming Exercise ( spiders/spidername.py )

  1 # -*- coding: utf-8 -*-
  2 import scrapy
  3 
  4 
  5 class SpidernameSpider(scrapy.Spider):
  6     name = 'spidername'
  7     allowed_domains = ['milemoa.com']
  8     start_urls = [ 'http://www.milemoa.com/bbs/' ]
  9 
 10 
 11     def parse(self, response):
 12     fn = response.url.split("/")[-2]
 13 
 14     with open(fn, 'wb') as f:
 15       f.write(response.body)
 16 
 17     pass
 18 
 19 
 20 pass
 
 3) $ scrapy list 		# list available spider bots and debugging source

 4) $ scrapy crawl spidername
2017-11-20 00:54:45 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: ProjectName)
2017-11-20 00:54:45 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'ProjectName.spiders', 'SPIDER_MODULES': ['ProjectName.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'ProjectName'}
...
2017-11-20 00:54:45 [scrapy.core.engine] INFO: Spider opened
2017-11-20 00:54:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 00:54:45 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-11-20 00:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.milemoa.com/robots.txt> (referer: None)
2017-11-20 00:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.milemoa.com/bbs/> (referer: None)
2017-11-20 00:54:46 [scrapy.core.engine] INFO: Closing spider (finished)
2017-11-20 00:54:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 440,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 11801,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 11, 19, 15, 54, 46, 522624),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 38502400,
 'memusage/startup': 38502400,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 11, 19, 15, 54, 45, 662503)}
2017-11-20 00:54:46 [scrapy.core.engine] INFO: Spider closed (finished)
 
----------------------------------------------------------------------------
* Global scrapy commands: 
	startproject / settings / runspider / shell / fetch / view / version
----------------------------------------------------------------------------

$ scrapy settings -h
----------------------------------------------------------------------------
Usage
=====
  scrapy settings [options]

Get settings values

Options
=======
--help, -h              show this help message and exit
--get=SETTING           print raw setting value
--getbool=SETTING       print setting value, interpreted as a boolean
--getint=SETTING        print setting value, interpreted as an integer
--getfloat=SETTING      print setting value, interpreted as a float
--getlist=SETTING       print setting value, interpreted as a list

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure


$ scrapy settings --get=BOT_NAME
----------------------------------------------------------------------------
ProjectName

$ scrapy runspider -h
----------------------------------------------------------------------------
Usage
=====
  scrapy runspider [options] <spider_file>

Run the spider defined in the given file

Options
=======
--help, -h              show this help message and exit
-a NAME=VALUE           set spider argument (may be repeated)
--output=FILE, -o FILE  dump scraped items into FILE (use - for stdout)
--output-format=FORMAT, -t FORMAT
                        format to use for dumping items with -o

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure


$pwd
/home/hadoop/scrapy/ProjectName/ProjectName/spiders
----------------------------------------------------------------------------

$ scrapy runspider spidername.py
----------------------------------------------------------------------------
2017-11-20 00:54:45 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: ProjectName)
2017-11-20 00:54:45 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'ProjectName.spiders', 'SPIDER_MODULES': ['ProjectName.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'ProjectName'}
...
2017-11-20 00:54:45 [scrapy.core.engine] INFO: Spider opened
2017-11-20 00:54:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 00:54:45 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-11-20 00:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.milemoa.com/robots.txt> (referer: None)
2017-11-20 00:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.milemoa.com/bbs/> (referer: None)
2017-11-20 00:54:46 [scrapy.core.engine] INFO: Closing spider (finished)
2017-11-20 00:54:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
...
2017-11-20 00:54:46 [scrapy.core.engine] INFO: Spider closed (finished)


$ scrapy fetch "http://www.naver.com"
----------------------------------------------------------------------------
2017-11-20 01:02:35 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: ProjectName)
...
2017-11-20 01:02:35 [scrapy.core.engine] INFO: Spider opened
2017-11-20 01:02:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:02:35 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-11-20 01:02:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.naver.com/robots.txt> from <GET http://www.naver.com/robots.txt>
2017-11-20 01:02:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.naver.com/robots.txt> (referer: None)
2017-11-20 01:02:36 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET http://www.naver.com> <---- ***** 
2017-11-20 01:02:36 [scrapy.core.engine] INFO: Closing spider (finished)
2017-11-20 01:02:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
...
2017-11-20 01:02:36 [scrapy.core.engine] INFO: Spider closed (finished)

$ scrapy fetch --nolog "http://www.milemoa.com/bbs/"
----------------------------------------------------------------------------
...


$ scrapy fetch --nolog --headers "http://www.milemoa.com/bbs/"
----------------------------------------------------------------------------
> Accept-Language: en
> Accept-Encoding: gzip,deflate
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> User-Agent: Scrapy/1.4.0 (+http://scrapy.org)
>
< X-Xss-Protection: 1; mode=block
< X-Content-Type-Options: nosniff
< Set-Cookie: PHPSESSID=93qrucub8v4t81v67m9sb6ha97; path=/
< Set-Cookie: mobile=false; path=/bbs/
< Set-Cookie: user-agent=b448996548621eba8b68446b251ebe01; path=/bbs/
< Expires: Mon, 26 Jul 1997 05:00:00 GMT
< X-Server-Powered-By: Engintron
< X-Nginx-Cache-Status: EXPIRED
< Server: nginx
< Last-Modified: Sun, 19 Nov 2017 16:07:46 GMT
< Pragma: no-cache
< Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0
< Date: Sun, 19 Nov 2017 16:07:46 GMT
< Vary: Accept-Encoding
< Content-Type: text/html; charset=UTF-8


$ scrapy shell "http://www.milemoa.com/bbs/" --nolog
----------------------------------------------------------------------------
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x3903bd0>
[s]   item       {}
[s]   request    <GET http://www.milemoa.com/bbs/>
[s]   response   <200 http://www.milemoa.com/bbs/>
[s]   settings   <scrapy.settings.Settings object at 0x3903a50>
[s]   spider     <SpidernameSpider 'spidername' at 0x7fc4b0010e50>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>>
>>> request
----------------------------------------------------------------------------
<200 http://www.milemoa.com/bbs/>
>>> 
>>> request.headers
----------------------------------------------------------------------------
{'Cookie': ['user-agent=b448996548621eba8b68446b251ebe01; PHPSESSID=n5pesbpf4gokdp27259o96h8g7'], 'Accept-Language': ['en'], 'Accept-Encoding': ['gzip,deflate'], 'Accept': ['text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'], 'User-Agent': ['Scrapy/1.4.0 (+http://scrapy.org)']}
>>>
>>> request.body
----------------------------------------------------------------------------
''
>>> 
>>> response
----------------------------------------------------------------------------
<200 http://www.milemoa.com/bbs/>
>>> 
>>> response.headers
----------------------------------------------------------------------------
{'X-Xss-Protection': ['1; mode=block'], 'X-Content-Type-Options': ['nosniff'], 'Set-Cookie': ['mobile=false; path=/bbs/'], 'Expires': ['Mon, 26 Jul 1997 05:00:00 GMT'], 'X-Server-Powered-By': ['Engintron'], 'X-Nginx-Cache-Status': ['EXPIRED'], 'Server': ['nginx'], 'Last-Modified': ['Sun, 19 Nov 2017 16:14:31 GMT'], 'Pragma': ['no-cache'], 'Cache-Control': ['no-store, no-cache, must-revalidate, post-check=0, pre-check=0'], 'Date': ['Sun, 19 Nov 2017 16:14:31 GMT'], 'Vary': ['Accept-Encoding'], 'Content-Type': ['text/html; charset=UTF-8']}
>>> 
>>> response.body
----------------------------------------------------------------------------
....
....
>>> 

 
----------------------------------------------------------------------------
* Project scrapy commands: 
	crawl / check / list / edit / genspider / bench
----------------------------------------------------------------------------

$ scrapy list
spidername
----------------------------------------------------------------------------

$ scrapy crawl spidername
----------------------------------------------------------------------------
2017-11-20 01:21:36 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: ProjectName)
...
2017-11-20 01:21:36 [scrapy.core.engine] INFO: Spider opened
2017-11-20 01:21:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:21:36 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-11-20 01:21:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.milemoa.com/robots.txt> (referer: None)
2017-11-20 01:21:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.milemoa.com/bbs/> (referer: None)
2017-11-20 01:21:37 [scrapy.core.engine] INFO: Closing spider (finished)
...
2017-11-20 01:21:37 [scrapy.core.engine] INFO: Spider closed (finished)

$ scrapy check
----------------------------------------------------------------------
Ran 0 contracts in 0.000s

OK

$ scrapy edit spidername
----------------------------------------------------------------------------
      1 # -*- coding: utf-8 -*-
      2 import scrapy
      3
      4
      5 class SpidernameSpider(scrapy.Spider):
      6     name = 'spidername'
      7     allowed_domains = ['milemoa.com']
      8     start_urls = [ 'http://www.milemoa.com/bbs/' ]
      9
     10
     11     def parse(self, response):
     12         fn = response.url.split("/")[-2]
     13
     14         with open(fn, 'wb') as f:
     15             f.write(response.body)
     16
     17         pass
     18
     19
     20 pass
	 
$ scrapy genspider -list or -l		# List available templates
----------------------------------------------------------------------------

$ scrapy genspider -d basic		or 	--dump=basic
----------------------------------------------------------------------------

$ scrapy genspider -d crawl		or 	--dump=crawl
----------------------------------------------------------------------------

$ scrapy genspider -d csvfeed	or 	--dump=csvfeed
----------------------------------------------------------------------------

$ scrapy genspider -d xmlfeed	or	--dump=xmlfeed
----------------------------------------------------------------------------

$ scrapy genspider --force --template=basic spidername milemoa.com		# Edit spider after creating it
$ scrapy genspider --force -t basic spidername milemoa.com

$ cd /home/hadoop/scrapy/	(NOK)
$ cd /home/hadoop/scrapy/ProjectName/	(ok)
$ cd /home/hadoop/scrapy/ProjectName/ProjectName/	(ok)
$ cd /home/hadoop/scrapy/ProjectName/ProjectName/spiders/	(ok)
----------------------------------------------------------------------------

$ scrapy bench
----------------------------------------------------------------------------
2017-11-20 01:28:02 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: ProjectName)
...
2017-11-20 01:28:02 [scrapy.core.engine] INFO: Spider opened
2017-11-20 01:28:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:03 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 5280 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:04 [scrapy.extensions.logstats] INFO: Crawled 197 pages (at 6540 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:05 [scrapy.extensions.logstats] INFO: Crawled 296 pages (at 5940 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:06 [scrapy.extensions.logstats] INFO: Crawled 371 pages (at 4500 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:07 [scrapy.extensions.logstats] INFO: Crawled 480 pages (at 6540 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:08 [scrapy.extensions.logstats] INFO: Crawled 561 pages (at 4860 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:09 [scrapy.extensions.logstats] INFO: Crawled 652 pages (at 5460 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:10 [scrapy.extensions.logstats] INFO: Crawled 752 pages (at 6000 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:11 [scrapy.extensions.logstats] INFO: Crawled 836 pages (at 5040 pages/min), scraped 0 items (at 0 items/min)
2017-11-20 01:28:12 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)
2017-11-20 01:28:12 [scrapy.extensions.logstats] INFO: Crawled 902 pages (at 3960 pages/min), scraped 0 items (at 0 items/min)
...
2017-11-20 01:28:12 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)


----------------------------------------------------------------------------
* Selector using XPath ( xpath for xml, HtmlXPathSelector for html)
  Caution) Selector always return a LIST including all return values
----------------------------------------------------------------------------

$ scrapy shell "http://www.milemoa.com/bbs/" --nolog
----------------------------------------------------------------------------
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x3951bd0>
[s]   item       {}
[s]   request    <GET http://www.milemoa.com/bbs/>
[s]   response   <200 http://www.milemoa.com/bbs/>
[s]   settings   <scrapy.settings.Settings object at 0x3951a50>
[s]   spider     <SpidernameSpider 'spidername' at 0x7f81d0010e50>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>> 
>>> req = request
----------------------------------------------------------------------------

>>> res = response
----------------------------------------------------------------------------

>>> res.xpath( '//table' )
----------------------------------------------------------------------------

>>> type(res.selector)
----------------------------------------------------------------------------
<class 'scrapy.selector.unified.Selector'>
----------------------------------------------------------------------------

>>> res.selector.xpath( '//table' )
----------------------------------------------------------------------------
[<Selector xpath='//table' data=u'<table id="lb_index" cellspacing="0" cel'>]

>>> res.xpath( '//table' )
----------------------------------------------------------------------------
[<Selector xpath='//table' data=u'<table id="lb_index" cellspacing="0" cel'>]

>>> res.xpath( '//table/text()' )	# extract text between the specified tags (<start>TEXT</end>)
----------------------------------------------------------------------------
[<Selector xpath='//table/text()' data=u'\n\t\t'>, <Selector xpath='//table/text()' data=u'\n\t\t'>, <Selector xpath='//table/text()' data=u'\n\t\t'>, <Selector xpath='//table/text()' data=u'\t\t\t\t\t\t\t'>, <Selector xpath='//table/text()' data=u'\t\t\t\t\t\t\t\t\t'>]

>>> res.xpath( '//title/text()' )	# extract text between the specified tags (<start>TEXT</end>)
----------------------------------------------------------------------------
[<Selector xpath='//title/text()' data=u'\ub9c8\uc77c\ubaa8\uc544 \uac8c\uc2dc\ud310'>]

>>> res.xpath( '//title/text()' ).extract()		# extract data from Selector objects
----------------------------------------------------------------------------
[u'\ub9c8\uc77c\ubaa8\uc544 \uac8c\uc2dc\ud310']

>>> res.xpath( '//table/@id' )		# extract value of the specified tags's attribute
----------------------------------------------------------------------------
[<Selector xpath='//table/@id' data=u'lb_index'>]

>>> res.xpath( '//table/@id' ).extract()		# extract data from Selector objects
----------------------------------------------------------------------------
[u'lb_index']

>>>
>>> res.xpath( '//table[@id="lb_index"]' )
----------------------------------------------------------------------------
[<Selector xpath='//table[@id="lb_index"]' data=u'<table id="lb_index" cellspacing="0" cel'>]

>>> res.xpath( '//table[@id="lb_index"]/tbody' )
----------------------------------------------------------------------------
[<Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-notice lb-in-1">\n\t\t\t\t\t\t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-notice">\n\t\t\t\t\t\t<tr>\n\t<t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-notice">\n\t\t\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-notice">\n\t\t\t\t\t\t<tr>\n\t<t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t\t\t\t\t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t\t\t\t\t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t\t\t\t\t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t\t\t\t\t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t\t\t\t\t'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>, <Selector xpath='//table[@id="lb_index"]/tbody' data=u'<tbody class="lb-document">\n\t\t\t\t\t\t\t\t<tr>'>]

>>> titles = res. \
... xpath( '//table[@id="lb_index"]' ). \
... xpath( './tbody[@class="lb-document"]' ). \
... xpath( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' ). \
... xpath( './/a[@class="lb-in-title lb-link"]/text()' ). \
... extract()
----------------------------------------------------------------------------
[u'\n\t\t\t\t\t\t\t\t\t\t\t\tHSA \uacc4\uc88c\uad00\ub828 \ub3c4\uc6c0 \ud544\uc694 \ud569\ub2c8\ub2e4.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ubc29\uae08 \uc774\uba54\uc77c\ub85c IHG SPIRE \ud61c\ud0dd\uc774 \uc654\ub124\uc694.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc815\uc804\uae30 \uc798\ub098\uc2dc\ub294\ubd84\u315c\u315c\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ub300\ud55c\ud56d\uacf5\uacfc \ub378\ud0c0\uac00 joint venture \ud558\uae30\ub85c \ud588\ub2e4\ub294\ub370\uc694\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2e0\ud63c\ubd80\ubd80\uc5d0\uac8c \ucd94\ucc9c\ud574\uc8fc\uc2e4\ub9cc\ud55c \uac8c\uc784\uae30!?\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t524 \uadf8\ub9ac\uace0 united 4\ub9cc\uc624\ud37c~\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tCenturion lounge LGA and MIA\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc8fc\uc2dd \ub300\ubc15-4\uac1c\uc6d4\uc5d0 100% \uc218\uc775\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2e0\ud63c\uc5ec\ud589 \uc608\uc57d\uc774 \ub4dc\ub514\uc5b4 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4!\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tGood or Bad deal? T-mobile Iphone 8 BOGO\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t4\ub300 \ud1b5\uc2e0\uc0ac  LG Deal\uc785\ub2c8\ub2e4. \uc5d8\uc9c0 \uc81c\ud488 \uad6c\ub9e4\uc2dc\uc5d0\ub9cc \ucd5c\uace0 $400 \ub9ac\ubca0\uc774\ud2b8.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t[\uc0ac\uc9c4\ud55c\uc7a5] \uc18c\uc0b4\ub9ac\ud1a0..\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2dc\ud2f0 \ud50c\ub798\ud2f0\ub118\uc744 \uc2e0\uccad\ud574\ub3c4 \ub420\uae4c\uc694? \uadf8\uc678 \uc18c\uc18c\ud55c \uc9c8\ubb38\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t[20140415\uc5c5\ub383] AMEX\uc758 \uac01\uc885 Protection Benefits\uc744 Claim\ud558\ub294 \ubc29\ubc95\t\t\t\t\t', u"\n\t\t\t\t\t\t\t\t\t\t\t\tAAFES Veteran's Day $20 or $50 Off Coupons. Nintendo Switch $249 or $239, Bose QuietComfort 35 $199 or $189 etc\t\t\t\t\t", u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2dc\uce74\uace0-\ud0c0\uc774\ud398\uc774-\uc778\ucc9c \ub9c8\uc77c\ub9ac\uc9c0 \ubc1c\uad8c \uc9c8\ubb38\ub4dc\ub824\uc694\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ubbf8\uad6d\uc5d0\uc11c \ud578\ub4dc\ud3f0 \uc0ac\uc6a9\uc740 \ud504\ub9ac\ud398\uc774\ub4dc\ub85c \uc0ac\uc6a9\ud558\ub294\uac8c \uc88b\ub098\uc694?\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ubab0\ub514\ube0c \ubc1c\uad8c \uc9c8\ubb38\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc774\ubc88 \ube14\ud504\uc5d0 \uac00\uc131\ube44 \uc88b\uac8c + \ub9cc\uc871\uc2a4\ub7fd\uac8c \uc0b4\ub9cc\ud55c \ud3f0\uc740? Pixel 2? Iphone X?\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ub77c\uc2a4\ubca0\uac00\uc2a4 \ucd94\ucc9c \uc1fc - \uc11c\ucee4\uc2a4 1903 (Circus 1903)\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t', u'\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tkungfu tea (\ubc84\ube14\ud2f0) \uc571 \ub2e4\uc6b4\ubc1b\uace0 $8 credit  \ubc1b\uc73c\uc138\uc694~~\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t[11/14 UPDATE] Verizon smart rewards \uae30\uce74 $100\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tcharles schwab \uc4f0\uc2dc\ub294 \ubd84\ub4e4\uc694 \uae09\ud55c \uc9c8\ubb38\uc774 \uc788\uc5b4\uc694.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc5bc\ub9c8\uc608\uc694 \uc5c4\uc120 \ud14c\ub808\ube44 \ub51c \ubaa8\uc74c:\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc544\ub9c8\uc874 \ube14\ub799 \ud504\ub77c\uc774\ub370\uc774 \ub51c\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tUnited credit \uc0ac\uc6a9 rebooking \uac00\uaca9\uc774 online \ud558\uace0 \ub108\ubb34 \ucc28\uc774\ub098\uc694\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uace0\ubc31\ubd80\ubd80 (\ub4dc\ub77c\ub9c8)\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t10\uace0\uac1c \ub118\uace0 AA 5\ub9cc \ub9c8\uc77c \ubc1b\uae30 \ud504\ub85c\ubaa8\uc158\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\ttopcashback referral\t\t\t\t\t']

>>> links = res. \
... xpath( '//table[@id="lb_index"]' ). \
... xpath( './tbody[@class="lb-document"]' ). \
... xpath( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' ). \
... xpath( './/a[@class="lb-in-title lb-link"]/@href' ). \
... extract()
----------------------------------------------------------------------------
[u'https://www.milemoa.com/bbs/4285010', u'https://www.milemoa.com/bbs/4285356', u'https://www.milemoa.com/bbs/4283185', u'https://www.milemoa.com/bbs/4284964', u'https://www.milemoa.com/bbs/4284892', u'https://www.milemoa.com/bbs/4284741', u'https://www.milemoa.com/bbs/4284356', u'https://www.milemoa.com/bbs/4210674', u'https://www.milemoa.com/bbs/4285221', u'https://www.milemoa.com/bbs/4282380', u'https://www.milemoa.com/bbs/4285255', u'https://www.milemoa.com/bbs/4087971', u'https://www.milemoa.com/bbs/4282539', u'https://www.milemoa.com/bbs/1783050', u'https://www.milemoa.com/bbs/4282558', u'https://www.milemoa.com/bbs/4284069', u'https://www.milemoa.com/bbs/3477380', u'https://www.milemoa.com/bbs/4284889', u'https://www.milemoa.com/bbs/4283700', u'https://www.milemoa.com/bbs/4285110', u'https://www.milemoa.com/bbs/4283784', u'https://www.milemoa.com/bbs/3994871', u'https://www.milemoa.com/bbs/4238051', u'https://www.milemoa.com/bbs/3298262', u'https://www.milemoa.com/bbs/4217516', u'https://www.milemoa.com/bbs/4280090', u'https://www.milemoa.com/bbs/4283773', u'https://www.milemoa.com/bbs/4228160', u'https://www.milemoa.com/bbs/4284911', u'https://www.milemoa.com/bbs/3132629']

>>> descs = res. \
... xpath( '//table[@id="lb_index"]' ). \
... xpath( './tbody[@class="lb-document"]' ). \
... xpath( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' ). \
... xpath( './/a[@class="lb-in-title lb-link"]/@title' ). \
... extract()
----------------------------------------------------------------------------
[u'HSA \uacc4\uc88c\uad00\ub828 \ub3c4\uc6c0 \ud544\uc694 \ud569\ub2c8\ub2e4.', u'\ubc29\uae08 \uc774\uba54\uc77c\ub85c IHG SPIRE \ud61c\ud0dd\uc774 \uc654\ub124\uc694.', u'\uc815\uc804\uae30 \uc798\ub098\uc2dc\ub294\ubd84\u315c\u315c', u'\ub300\ud55c\ud56d\uacf5\uacfc \ub378\ud0c0\uac00 joint venture \ud558\uae30\ub85c \ud588\ub2e4\ub294\ub370\uc694', u'\uc2e0\ud63c\ubd80\ubd80\uc5d0\uac8c \ucd94\ucc9c\ud574\uc8fc\uc2e4\ub9cc\ud55c \uac8c\uc784\uae30!?', u'524 \uadf8\ub9ac\uace0 united 4\ub9cc\uc624\ud37c~', u'Centurion lounge LGA and MIA', u'\uc8fc\uc2dd \ub300\ubc15-4\uac1c\uc6d4\uc5d0 100% \uc218\uc775', u'\uc2e0\ud63c\uc5ec\ud589 \uc608\uc57d\uc774 \ub4dc\ub514\uc5b4 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4!', u'Good or Bad deal? T-mobile Iphone 8 BOGO', u'4\ub300 \ud1b5\uc2e0\uc0ac  LG Deal\uc785\ub2c8\ub2e4. \uc5d8\uc9c0 \uc81c\ud488 \uad6c\ub9e4\uc2dc\uc5d0\ub9cc \ucd5c\uace0 $400 \ub9ac\ubca0\uc774\ud2b8.', u'[\uc0ac\uc9c4\ud55c\uc7a5] \uc18c\uc0b4\ub9ac\ud1a0..', u'\uc2dc\ud2f0 \ud50c\ub798\ud2f0\ub118\uc744 \uc2e0\uccad\ud574\ub3c4 \ub420\uae4c\uc694? \uadf8\uc678 \uc18c\uc18c\ud55c \uc9c8\ubb38', u'[20140415\uc5c5\ub383] AMEX\uc758 \uac01\uc885 Protection Benefits\uc744 Claim\ud558\ub294 \ubc29\ubc95', u"AAFES Veteran's Day $20 or $50 Off Coupons. Nintendo Switch $249 or $239, Bose QuietComfort 35 $199 or $189 etc", u'\uc2dc\uce74\uace0-\ud0c0\uc774\ud398\uc774-\uc778\ucc9c \ub9c8\uc77c\ub9ac\uc9c0 \ubc1c\uad8c \uc9c8\ubb38\ub4dc\ub824\uc694', u'\ubbf8\uad6d\uc5d0\uc11c \ud578\ub4dc\ud3f0 \uc0ac\uc6a9\uc740 \ud504\ub9ac\ud398\uc774\ub4dc\ub85c \uc0ac\uc6a9\ud558\ub294\uac8c \uc88b\ub098\uc694?', u'\ubab0\ub514\ube0c \ubc1c\uad8c \uc9c8\ubb38', u'\uc774\ubc88 \ube14\ud504\uc5d0 \uac00\uc131\ube44 \uc88b\uac8c + \ub9cc\uc871\uc2a4\ub7fd\uac8c \uc0b4\ub9cc\ud55c \ud3f0\uc740? Pixel 2? Iphone X?', u'\ub77c\uc2a4\ubca0\uac00\uc2a4 \ucd94\ucc9c \uc1fc - \uc11c\ucee4\uc2a4 1903 (Circus 1903)', u'\uc774 \uae00\uc774 \ubcf4\uc774\uc2e0\ub2e4\uba74 \uc0c8 \uc11c\ubc84\uc5d0 \uc798 \ucc3e\uc544\uc624\uc2e0 \uac81\ub2c8\ub2e4', u'kungfu tea (\ubc84\ube14\ud2f0) \uc571 \ub2e4\uc6b4\ubc1b\uace0 $8 credit  \ubc1b\uc73c\uc138\uc694~~', u'[11/14 UPDATE] Verizon smart rewards \uae30\uce74 $100', u'charles schwab \uc4f0\uc2dc\ub294 \ubd84\ub4e4\uc694 \uae09\ud55c \uc9c8\ubb38\uc774 \uc788\uc5b4\uc694.', u'\uc5bc\ub9c8\uc608\uc694 \uc5c4\uc120 \ud14c\ub808\ube44 \ub51c \ubaa8\uc74c:', u'\uc544\ub9c8\uc874 \ube14\ub799 \ud504\ub77c\uc774\ub370\uc774 \ub51c', u'United credit \uc0ac\uc6a9 rebooking \uac00\uaca9\uc774 online \ud558\uace0 \ub108\ubb34 \ucc28\uc774\ub098\uc694', u'\uace0\ubc31\ubd80\ubd80 (\ub4dc\ub77c\ub9c8)', u'10\uace0\uac1c \ub118\uace0 AA 5\ub9cc \ub9c8\uc77c \ubc1b\uae30 \ud504\ub85c\ubaa8\uc158', u'topcashback referral']
>>> 
>>> from scrapy.selector import HtmlXPathSelector
----------------------------------------------------------------------------
>>> hxs = HtmlXPathSelector( res )
----------------------------------------------------------------------------
>>> type( hxs )
----------------------------------------------------------------------------
<class 'scrapy.selector.lxmlsel.HtmlXPathSelector'>

>>> titles = hxs. \
... select( '//table[@id="lb_index"]' ). \
... select( './tbody[@class="lb-document"]' ). \
... select( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' ). \
... select( './/a[@class="lb-in-title lb-link"]/text()' ). \
... extract()
----------------------------------------------------------------------------
>>>  titles
----------------------------------------------------------------------------
[u'\n\t\t\t\t\t\t\t\t\t\t\t\tHSA \uacc4\uc88c\uad00\ub828 \ub3c4\uc6c0 \ud544\uc694 \ud569\ub2c8\ub2e4.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ubc29\uae08 \uc774\uba54\uc77c\ub85c IHG SPIRE \ud61c\ud0dd\uc774 \uc654\ub124\uc694.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc815\uc804\uae30 \uc798\ub098\uc2dc\ub294\ubd84\u315c\u315c\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ub300\ud55c\ud56d\uacf5\uacfc \ub378\ud0c0\uac00 joint venture \ud558\uae30\ub85c \ud588\ub2e4\ub294\ub370\uc694\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2e0\ud63c\ubd80\ubd80\uc5d0\uac8c \ucd94\ucc9c\ud574\uc8fc\uc2e4\ub9cc\ud55c \uac8c\uc784\uae30!?\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t524 \uadf8\ub9ac\uace0 united 4\ub9cc\uc624\ud37c~\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tCenturion lounge LGA and MIA\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc8fc\uc2dd \ub300\ubc15-4\uac1c\uc6d4\uc5d0 100% \uc218\uc775\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2e0\ud63c\uc5ec\ud589 \uc608\uc57d\uc774 \ub4dc\ub514\uc5b4 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4!\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tGood or Bad deal? T-mobile Iphone 8 BOGO\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t4\ub300 \ud1b5\uc2e0\uc0ac  LG Deal\uc785\ub2c8\ub2e4. \uc5d8\uc9c0 \uc81c\ud488 \uad6c\ub9e4\uc2dc\uc5d0\ub9cc \ucd5c\uace0 $400 \ub9ac\ubca0\uc774\ud2b8.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t[\uc0ac\uc9c4\ud55c\uc7a5] \uc18c\uc0b4\ub9ac\ud1a0..\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2dc\ud2f0 \ud50c\ub798\ud2f0\ub118\uc744 \uc2e0\uccad\ud574\ub3c4 \ub420\uae4c\uc694? \uadf8\uc678 \uc18c\uc18c\ud55c \uc9c8\ubb38\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t[20140415\uc5c5\ub383] AMEX\uc758 \uac01\uc885 Protection Benefits\uc744 Claim\ud558\ub294 \ubc29\ubc95\t\t\t\t\t', u"\n\t\t\t\t\t\t\t\t\t\t\t\tAAFES Veteran's Day $20 or $50 Off Coupons. Nintendo Switch $249 or $239, Bose QuietComfort 35 $199 or $189 etc\t\t\t\t\t", u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc2dc\uce74\uace0-\ud0c0\uc774\ud398\uc774-\uc778\ucc9c \ub9c8\uc77c\ub9ac\uc9c0 \ubc1c\uad8c \uc9c8\ubb38\ub4dc\ub824\uc694\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ubbf8\uad6d\uc5d0\uc11c \ud578\ub4dc\ud3f0 \uc0ac\uc6a9\uc740 \ud504\ub9ac\ud398\uc774\ub4dc\ub85c \uc0ac\uc6a9\ud558\ub294\uac8c \uc88b\ub098\uc694?\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ubab0\ub514\ube0c \ubc1c\uad8c \uc9c8\ubb38\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc774\ubc88 \ube14\ud504\uc5d0 \uac00\uc131\ube44 \uc88b\uac8c + \ub9cc\uc871\uc2a4\ub7fd\uac8c \uc0b4\ub9cc\ud55c \ud3f0\uc740? Pixel 2? Iphone X?\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\ub77c\uc2a4\ubca0\uac00\uc2a4 \ucd94\ucc9c \uc1fc - \uc11c\ucee4\uc2a4 1903 (Circus 1903)\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t', u'\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tkungfu tea (\ubc84\ube14\ud2f0) \uc571 \ub2e4\uc6b4\ubc1b\uace0 $8 credit  \ubc1b\uc73c\uc138\uc694~~\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t[11/14 UPDATE] Verizon smart rewards \uae30\uce74 $100\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tcharles schwab \uc4f0\uc2dc\ub294 \ubd84\ub4e4\uc694 \uae09\ud55c \uc9c8\ubb38\uc774 \uc788\uc5b4\uc694.\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc5bc\ub9c8\uc608\uc694 \uc5c4\uc120 \ud14c\ub808\ube44 \ub51c \ubaa8\uc74c:\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uc544\ub9c8\uc874 \ube14\ub799 \ud504\ub77c\uc774\ub370\uc774 \ub51c\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\tUnited credit \uc0ac\uc6a9 rebooking \uac00\uaca9\uc774 online \ud558\uace0 \ub108\ubb34 \ucc28\uc774\ub098\uc694\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t\uace0\ubc31\ubd80\ubd80 (\ub4dc\ub77c\ub9c8)\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\t10\uace0\uac1c \ub118\uace0 AA 5\ub9cc \ub9c8\uc77c \ubc1b\uae30 \ud504\ub85c\ubaa8\uc158\t\t\t\t\t', u'\n\t\t\t\t\t\t\t\t\t\t\t\ttopcashback referral\t\t\t\t\t']

>>> links = hxs. \
... select( '//table[@id="lb_index"]' ). \
... select( './tbody[@class="lb-document"]' ). \
... select( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' ). \
... select( './/a[@class="lb-in-title lb-link"]/@href' ). \
... extract()
----------------------------------------------------------------------------
>>> links
----------------------------------------------------------------------------
[u'https://www.milemoa.com/bbs/4285010', u'https://www.milemoa.com/bbs/4285356', u'https://www.milemoa.com/bbs/4283185', u'https://www.milemoa.com/bbs/4284964', u'https://www.milemoa.com/bbs/4284892', u'https://www.milemoa.com/bbs/4284741', u'https://www.milemoa.com/bbs/4284356', u'https://www.milemoa.com/bbs/4210674', u'https://www.milemoa.com/bbs/4285221', u'https://www.milemoa.com/bbs/4282380', u'https://www.milemoa.com/bbs/4285255', u'https://www.milemoa.com/bbs/4087971', u'https://www.milemoa.com/bbs/4282539', u'https://www.milemoa.com/bbs/1783050', u'https://www.milemoa.com/bbs/4282558', u'https://www.milemoa.com/bbs/4284069', u'https://www.milemoa.com/bbs/3477380', u'https://www.milemoa.com/bbs/4284889', u'https://www.milemoa.com/bbs/4283700', u'https://www.milemoa.com/bbs/4285110', u'https://www.milemoa.com/bbs/4283784', u'https://www.milemoa.com/bbs/3994871', u'https://www.milemoa.com/bbs/4238051', u'https://www.milemoa.com/bbs/3298262', u'https://www.milemoa.com/bbs/4217516', u'https://www.milemoa.com/bbs/4280090', u'https://www.milemoa.com/bbs/4283773', u'https://www.milemoa.com/bbs/4228160', u'https://www.milemoa.com/bbs/4284911', u'https://www.milemoa.com/bbs/3132629']

>>> descs = hxs. \
... select( '//table[@id="lb_index"]' ). \
... select( './tbody[@class="lb-document"]' ). \
... select( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' ). \
... select( './/a[@class="lb-in-title lb-link"]/@title' ). \
... extract()
----------------------------------------------------------------------------
>>> descs
----------------------------------------------------------------------------
[u'HSA \uacc4\uc88c\uad00\ub828 \ub3c4\uc6c0 \ud544\uc694 \ud569\ub2c8\ub2e4.', u'\ubc29\uae08 \uc774\uba54\uc77c\ub85c IHG SPIRE \ud61c\ud0dd\uc774 \uc654\ub124\uc694.', u'\uc815\uc804\uae30 \uc798\ub098\uc2dc\ub294\ubd84\u315c\u315c', u'\ub300\ud55c\ud56d\uacf5\uacfc \ub378\ud0c0\uac00 joint venture \ud558\uae30\ub85c \ud588\ub2e4\ub294\ub370\uc694', u'\uc2e0\ud63c\ubd80\ubd80\uc5d0\uac8c \ucd94\ucc9c\ud574\uc8fc\uc2e4\ub9cc\ud55c \uac8c\uc784\uae30!?', u'524 \uadf8\ub9ac\uace0 united 4\ub9cc\uc624\ud37c~', u'Centurion lounge LGA and MIA', u'\uc8fc\uc2dd \ub300\ubc15-4\uac1c\uc6d4\uc5d0 100% \uc218\uc775', u'\uc2e0\ud63c\uc5ec\ud589 \uc608\uc57d\uc774 \ub4dc\ub514\uc5b4 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4!', u'Good or Bad deal? T-mobile Iphone 8 BOGO', u'4\ub300 \ud1b5\uc2e0\uc0ac  LG Deal\uc785\ub2c8\ub2e4. \uc5d8\uc9c0 \uc81c\ud488 \uad6c\ub9e4\uc2dc\uc5d0\ub9cc \ucd5c\uace0 $400 \ub9ac\ubca0\uc774\ud2b8.', u'[\uc0ac\uc9c4\ud55c\uc7a5] \uc18c\uc0b4\ub9ac\ud1a0..', u'\uc2dc\ud2f0 \ud50c\ub798\ud2f0\ub118\uc744 \uc2e0\uccad\ud574\ub3c4 \ub420\uae4c\uc694? \uadf8\uc678 \uc18c\uc18c\ud55c \uc9c8\ubb38', u'[20140415\uc5c5\ub383] AMEX\uc758 \uac01\uc885 Protection Benefits\uc744 Claim\ud558\ub294 \ubc29\ubc95', u"AAFES Veteran's Day $20 or $50 Off Coupons. Nintendo Switch $249 or $239, Bose QuietComfort 35 $199 or $189 etc", u'\uc2dc\uce74\uace0-\ud0c0\uc774\ud398\uc774-\uc778\ucc9c \ub9c8\uc77c\ub9ac\uc9c0 \ubc1c\uad8c \uc9c8\ubb38\ub4dc\ub824\uc694', u'\ubbf8\uad6d\uc5d0\uc11c \ud578\ub4dc\ud3f0 \uc0ac\uc6a9\uc740 \ud504\ub9ac\ud398\uc774\ub4dc\ub85c \uc0ac\uc6a9\ud558\ub294\uac8c \uc88b\ub098\uc694?', u'\ubab0\ub514\ube0c \ubc1c\uad8c \uc9c8\ubb38', u'\uc774\ubc88 \ube14\ud504\uc5d0 \uac00\uc131\ube44 \uc88b\uac8c + \ub9cc\uc871\uc2a4\ub7fd\uac8c \uc0b4\ub9cc\ud55c \ud3f0\uc740? Pixel 2? Iphone X?', u'\ub77c\uc2a4\ubca0\uac00\uc2a4 \ucd94\ucc9c \uc1fc - \uc11c\ucee4\uc2a4 1903 (Circus 1903)', u'\uc774 \uae00\uc774 \ubcf4\uc774\uc2e0\ub2e4\uba74 \uc0c8 \uc11c\ubc84\uc5d0 \uc798 \ucc3e\uc544\uc624\uc2e0 \uac81\ub2c8\ub2e4', u'kungfu tea (\ubc84\ube14\ud2f0) \uc571 \ub2e4\uc6b4\ubc1b\uace0 $8 credit  \ubc1b\uc73c\uc138\uc694~~', u'[11/14 UPDATE] Verizon smart rewards \uae30\uce74 $100', u'charles schwab \uc4f0\uc2dc\ub294 \ubd84\ub4e4\uc694 \uae09\ud55c \uc9c8\ubb38\uc774 \uc788\uc5b4\uc694.', u'\uc5bc\ub9c8\uc608\uc694 \uc5c4\uc120 \ud14c\ub808\ube44 \ub51c \ubaa8\uc74c:', u'\uc544\ub9c8\uc874 \ube14\ub799 \ud504\ub77c\uc774\ub370\uc774 \ub51c', u'United credit \uc0ac\uc6a9 rebooking \uac00\uaca9\uc774 online \ud558\uace0 \ub108\ubb34 \ucc28\uc774\ub098\uc694', u'\uace0\ubc31\ubd80\ubd80 (\ub4dc\ub77c\ub9c8)', u'10\uace0\uac1c \ub118\uace0 AA 5\ub9cc \ub9c8\uc77c \ubc1b\uae30 \ud504\ub85c\ubaa8\uc158', u'topcashback referral']
>>> 
----------------------------------------------------------------------------
** Pagination : https://www.milemoa.com/bbs/index.php?mid=board&page=1 ~ 1811
----------------------------------------------------------------------------

  1 # -*- coding: utf-8 -*-
  2 import scrapy
  3 import time
  4 import re
  5
  6 from random import randint
  9 from ProjectName.items import ProjectnameItem
 10
 11 #-- Deprecated : from scrapy.contrib.spiders import CrawlSpider
 12 from scrapy.spiders import CrawlSpider
 13
 14 from scrapy.selector import HtmlXPathSelector
 15 from scrapy.http.request import Request
 16
 18 start_page = 1
 19 end_page = 1811
 20
 21
 22 class SpidernameSpider( CrawlSpider ):
 23     name = 'milemoa'
 24     allowed_domains = ['milemoa.com']
 25     start_urls = [ 'https://www.milemoa.com/bbs/index.php?mid=board&page=1' ]
 26
 27
 28     def parse(self, response):
 29         global start_page, end_page
 30
 31         hxs = HtmlXPathSelector( response )
 32         print '** HXS:', hxs
 33
 34         anchors = hxs.select( '//table[@id="lb_index"]' ).select( './tbody[@class="lb-document"]' ).select( './/td[@class="lb-in-td lb-title lb-in-no_webzine"]' )
 36         print
 37
 38         seq = 0
 39
 40         for A in anchors:
 41             seq += 1
 42
 43             print "** SEQ:", seq
 44
 45             item = ProjectnameItem()
 46
 48             item['title'] = A.select( './h3[@class="lb-in-title"]/a[@class="lb-in-title lb-link"]/text()' ).extract()[0]
 49             item['title'] = item['title'].replace("\t", "").replace("\n", "")
 50
 52             item['link'] = A.select( './h3[@class="lb-in-title"]/a[@class="lb-in-title lb-link"]/@href' ).extract()[0]
 55             item['desc'] = A.select( './h3[@class="lb-in-title"]/a[@class="lb-in-title lb-link"]/@title' ).extract()[0]
 58
 59             yield item
 60         else:
 61             sleepsec = randint(1, 5);
 62
 63             print '\t** <{}> Page ** Crawling Done. <{}> seconds got sleep -'.format( start_page, sleepsec )
 64
 65             time.sleep( sleepsec )
 66
 67             if( start_page <= end_page ):
 68                 start_page += 1
 69
 70                 next_urls = [ 'https://www.milemoa.com/bbs/index.php?mid=board&page=' + str(start_page) ]
 71                 yield Request( next_urls[0], self.parse )
 72
 73         pass
 74
 75
 76 pass
 
 
 $ scrapy list
 $ pwd
 /home/hadoop/scrapy/ProjectName/ProjectName/spiders
 $ scrapy crawl milemoa  -o milemoa.csv -t csv
 $ cd
 $ cd scrapy/ProjectName/ProjectName
 $ vi items.py
  14     title = scrapy.Field()
  15     link = scrapy.Field()
  16     desc = scrapy.Field()   추가
    

----------------------------------------------------------------------------
** Storing cs file into Hadoop HDFS
----------------------------------------------------------------------------

0. Apache Hadoop v1.2.1 HDFS start
----------------------------------------------------------------------------
$ start-dfs.sh


1. flume workflow for storing crawled data into Hadoop HDFS 
   ( /opt/flume/conf/flume-conf.properties )
----------------------------------------------------------------------------
  1 milemoa.sources = s1
  2 milemoa.channels = c1
  3 milemoa.sinks = k1 k2
  4
  5
  6 milemoa.sources.s1.type = exec
  7 milemoa.sources.s1.command = tail -f /home/hadoop/scrapy/ProjectName/ProjectName/spiders/milemoa.csv
  8 milemoa.sources.s1.channels = c1
  9
 10
 11 milemoa.sources.s1.interceptors = i1
 12 milemoa.sources.s1.interceptors.i1.type = timestamp
 13
 14
 15 milemoa.channels.c1.type = memory
 16 milemoa.channels.c1.capacity = 100
 17
 18
 19 milemoa.sinks.k1.type = logger
 20 milemoa.sinks.k1.channel = c1
 21
 22
 23 milemoa.sinks.k2.type = hdfs
 24 milemoa.sinks.k2.hdfs.path = hdfs://hadoop-master:9000/flume/milemoa/%y-%m-%d/%H%M/
 25 milemoa.sinks.k2.hdfs.writeFormat = Text
 26 milemoa.sinks.k2.channel = c1
 
 
2. flume agent start 
----------------------------------------------------------------------------
$ flume-ng \
> agent \
> --conf-file /opt/flume/conf/flume.conf.properties
> --name milemoa
> -Dflume.root.logger=INFO,console


3. Watching Hadoop HDFS directory: /flume 
----------------------------------------------------------------------------
$ watch -n 1 'hadoop fs -lsr /flume'


